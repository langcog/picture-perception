{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.random import Generator, PCG64\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393faf4",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b22f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoa_df = pd.read_csv(\"../data/word_age_of_acquisition.csv\")\n",
    "aoa_df = aoa_df.replace({\"comb (object)\": \"comb\"})\n",
    "df = pd.read_csv(\"../data/kisumu_vocab_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6995d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_classification = pd.read_csv(\"../data/word_classification_df.csv\")\n",
    "word_classification['Target'] = word_classification['Target'].apply(lambda w : w.lower())\n",
    "word_classification['Near Distractor'] = word_classification['Near Distractor'].apply(lambda w : w.lower())\n",
    "word_classification['Random 1'] = word_classification['Random 1'].apply(lambda w : w.lower())\n",
    "word_classification['Random 2'] = word_classification['Random 2'].apply(lambda w : w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a388ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(r):\n",
    "    target = r['target_word'].title()\n",
    "    word_response = subject_responses[\n",
    "        (subject_responses[\"Subject Number\"] == r[\"child\"])\n",
    "    ].iloc[0][target]\n",
    "\n",
    "    return word_response.lower()\n",
    "\n",
    "def classify_response(r):\n",
    "    res = r['response']\n",
    "    if res == r['Target']:\n",
    "        return 'Target'\n",
    "    elif res == r['Near Distractor']:\n",
    "        return 'Near Distractor'\n",
    "    elif res == r['Random 1'] or r['Random 2']:\n",
    "        return 'Random'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "subject_responses = pd.read_csv(\"../data/Kisumu_2024_Vocabulary_Vocab.csv\").dropna(subset=[\"Subject Number\"])\n",
    "all_df = pd.merge(df, word_classification, left_on=\"target_word\", right_on=\"Target\", how=\"left\")\n",
    "all_df = pd.merge(all_df, aoa_df, on='target_word', how='left')\n",
    "all_df['response'] = all_df.apply(find_word, axis=1)\n",
    "all_df['response_type'] = all_df.apply(classify_response, axis=1)\n",
    "all_df.to_csv(\"../data/all_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487c602",
   "metadata": {},
   "source": [
    "# Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = all_df.copy()\n",
    "filtered_df = filtered_df[filtered_df['accuracy'] == 0]\n",
    "filtered_df['is_near_distractor'] = (filtered_df['response_type'] == 'Near Distractor').astype(int)\n",
    "filtered_df['is_random'] = (filtered_df['response_type'] == 'Random').astype(int)\n",
    "filtered_df['condition'] = filtered_df['condition'].replace({\n",
    "    'bw': 'black_white',\n",
    "    'obj': 'object',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(\n",
    "        data,\n",
    "        measure,\n",
    "        id_col,\n",
    "        n_iterations=10000,\n",
    "        statistic=np.mean):\n",
    "    \n",
    "    items = list(data[id_col].unique())\n",
    "    n_size = len(items)\n",
    "    df = data.copy()\n",
    "\n",
    "    def bootstrap_iteration(data, chosen_items):\n",
    "        filter_df = data[data[id_col].isin(chosen_items)] # Filter based on chosen questions\n",
    "        bs_mean = statistic(filter_df[measure]) \n",
    "        return (bs_mean, list(chosen_items))\n",
    "\n",
    "    qset_means = Parallel(n_jobs=-1)(\n",
    "        delayed(bootstrap_iteration)(\n",
    "            df.copy(),\n",
    "            rng.choice(items, n_size,  replace=True)\n",
    "        ) for _ in range(n_iterations)\n",
    "    )\n",
    "    \n",
    "    means = []\n",
    "    qs_used = []\n",
    "    means = [bs_mean for bs_mean, chosen_qs in qset_means]\n",
    " \n",
    "    # 95% confidence interval\n",
    "    lower = np.percentile(means, 2.5)\n",
    "    upper = np.percentile(means, 97.5)\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "def create_confidence_interval_df(\n",
    "    data,\n",
    "    measure, \n",
    "    id_col,\n",
    "    condition_col,\n",
    "    statistic=np.mean\n",
    "):\n",
    "    data_list = []\n",
    "\n",
    "    for condition in data[condition_col].unique():\n",
    "        condition_data = data[data[condition_col] == condition]\n",
    "\n",
    "        lower, upper = bootstrap_ci(condition_data, measure=measure, statistic=statistic, id_col=id_col)\n",
    "\n",
    "        data_list.append({\n",
    "            \"condition\": condition,\n",
    "            \"ci_upper\": upper, \n",
    "            \"ci_lower\": lower,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a311af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_dfs(df, measure):\n",
    "    agg_data = df.groupby(['condition']).agg(\n",
    "        mean_prop=(measure, 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    word_level_data = df.groupby(['condition', 'target_word']).agg(\n",
    "        mean_prop=(measure, 'mean')\n",
    "    ).reset_index()\n",
    "    word_level_data['item_id'] = word_level_data['condition'] + word_level_data['target_word']\n",
    "    \n",
    "    ci_df = create_confidence_interval_df(\n",
    "        data=df,\n",
    "        measure=measure,\n",
    "        id_col='target_word',\n",
    "        condition_col='condition'\n",
    "    )\n",
    "\n",
    "    error_df = pd.merge(agg_data, ci_df, on=['condition'])\n",
    "\n",
    "    return error_df, word_level_data\n",
    "\n",
    "error_dfs, item_dfs = get_error_dfs(\n",
    "    filtered_df, \n",
    "    measure='is_random'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ef62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_error_plot(error_df, item_level_df):\n",
    "    ci_plot = alt.Chart(error_df).mark_errorbar().encode(\n",
    "        x=alt.X(\"condition:N\", title=None),\n",
    "        y=alt.Y(\"ci_upper\", title=\"Proportion of Far Distractors Selected\"),\n",
    "        y2=alt.Y2(\"ci_lower\"),\n",
    "        strokeWidth=alt.value(2),\n",
    "        color=alt.Color('condition').legend(None)\n",
    "    )\n",
    "\n",
    "    mean_points = alt.Chart(error_df).mark_point(filled=True, size=75, opacity=1).encode(\n",
    "        x=alt.X('condition:N', scale=alt.Scale(domain=[\"black_white\", 'cartoon', 'photo', 'object'])),\n",
    "        y=alt.Y('mean_prop:Q', scale=alt.Scale(domain=[0,1])),\n",
    "        color='condition:N'\n",
    "    )\n",
    "    \n",
    "    scatter_plot = alt.Chart(item_level_df).mark_circle(size=16,opacity=0.5).encode(\n",
    "        x=alt.X(\"condition:N\", title=None),\n",
    "        y=alt.Y(\"mean_prop:Q\", scale=alt.Scale(domain=[0,1])),\n",
    "        xOffset=\"jitter:Q\",\n",
    "        color=alt.Color('condition:N').legend(None),\n",
    "    ).transform_calculate(\n",
    "        jitter=\"sqrt(-2*log(random()))*cos(2*PI*random())\" \n",
    "    )\n",
    "    \n",
    "    dashed_line = alt.Chart(error_df).mark_rule(strokeDash=[5, 10], color='black').encode(\n",
    "        y=alt.datum(0.66),\n",
    "        opacity=alt.value(0.5)\n",
    "    )\n",
    "    \n",
    "    return mean_points + scatter_plot + ci_plot + dashed_line\n",
    "\n",
    "plot = create_error_plot(error_dfs, item_dfs)\n",
    "plot = plot.properties(width=150, title=\"Proportion of Incorrect Responses\")\n",
    "# plot.save(\"../figs/proportion_incorrect_plot.pdf\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc97c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "040ef1b4",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_selection_df = all_df.copy()\n",
    "all_selection_df = all_selection_df[all_selection_df['accuracy'] == 0]\n",
    "all_selection_df['is_near_distractor'] = (all_selection_df['response_type'] == 'Near Distractor').astype(int)\n",
    "all_selection_df['is_random'] = (all_selection_df['response_type'] == 'Random').astype(int)\n",
    "all_selection_df = all_selection_df[['child', 'condition', 'age', 'target_word', 'accuracy', 'response_type', 'is_near_distractor', 'is_random']]\n",
    "all_selection_df.to_csv(\"../data/error_trials_only.csv\")\n",
    "# all_selection_df.to_csv(\"../data/all_selection_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aff0d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45de929",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec81d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages(\"lme4\")\n",
    "library(lme4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72565c30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a462a97",
   "metadata": {},
   "source": [
    "The below is implemented in the `picture-perception-error-analysis.Rmd` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_selection_df\n",
    "\n",
    "base_model <- glmer(\n",
    "    is_random ~ 1  + (condition | child) + (condition * age | target_word), \n",
    "    data=all_selection_df, \n",
    "    family = \"binomial\"\n",
    ")\n",
    "\n",
    "condition_model <- glmer(\n",
    "    is_random ~ condition * age + (condition | child) + (condition * age | target_word), \n",
    "    data=all_selection_df, \n",
    "    family = \"binomial\"\n",
    ")\n",
    "\n",
    "anova(base_model, condition_model, test = \"Chisq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8786c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "base_model <- glmer(\n",
    "    is_random ~ 1  + (condition | child), \n",
    "    data=all_selection_df, \n",
    "    family = \"binomial\"\n",
    ")\n",
    "\n",
    "condition_model <- glmer(\n",
    "    is_random ~ condition + (condition | child), \n",
    "    data=all_selection_df, \n",
    "    family = \"binomial\"\n",
    ")\n",
    "\n",
    "anova(base_model, condition_model, test = \"Chisq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
